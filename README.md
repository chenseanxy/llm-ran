# llm-ran
LLM-RAN: Interactive Network Operations usingLarge Language Models in ORAN

## Project Structure

- `data`: input and output data files, small enough to fit in CSC's 50G `/projectappl` folder
    - `data/*.csv`: Raw CSVs generated by trials, contains expected answers and raw output from LLMs
    - `data/intermediate`: Intermediate files used during pre-processing, mainly for evaluating which answers are NaN
    - `data/processed`: Processed data files by question type, contains `can_answer` and `correct` fields
    - `data/snapshots`: Snapshots of LLM runs, contains all attributes and messages from an LLM trial
- `logs`
- `llm_ran`: main package
    - `llm`: Load models, define inference endpoints
    - `k8s`: Build Kubernetes agents, `codegen` and `direct`
    - `k8s_env`: Load Kubernetes scenarios
    - `benchmark`: Benchmark questions, and run benchmarks
    - `frontend`: Streamlit frontend for running the LLMs
- `deployment`: Docker and Apptainer related stuff
- `experiments`: Bootstrap scripts for running experiments
- `scripts`: scripts for submitting jobs and running experiments
- `notebooks`: notebooks for result analysis
    - `preprocessing.ipynb`: turn `data/*.csv` into `data/processed/*.csv`, add the `can_answer` and `correct` fields
    - Everything else: graphing, etc.

## Development

1. Setup Kubernetes cluster and cluster access, check `kubectl get nodes` to see if there's access to the cluster.
    - You can consider using [k3d](https://k3d.io/) for a small scale local cluster
2. Install cluster workloads with Kustomize: `kubectl apply -k llm_ran/k8s_env/manifests/base`
    - Wait for the pods to be ready: `kubectl get pods`
3. Install Python dependencies with [Poetry](https://python-poetry.org/): `poetry install`
4. Install [Ollama](https://ollama.com/) and pull models
5. Run experiments
    - You can run trials in parallel, but only within the same scenario
        - If you do this, it's recommended to not use `k8s_env.scenarios` to manage scenarios, but apply them manually by first modifying `llm_ran/k8s_env/manifests/kustomize.yaml` and then `kubectl apply -k llm_ran/k8s_env/manifests`
        - Use multiple console windows to run trials in parallel
        - It won't make it faster if you only have one GPU
    - Check progress in `logs/*_progress.log`, and individual trial results are also logged in `logs/*_experiment.log`
6. `notebooks/preprocessing.ipynb` for preprocessing, adding `can_answer` and `correct` fields to the trials
7. Use `notebooks/*.ipynb` for graphing.


## Deployment

Run experiments locally:

`poetry install && poetry run python experiments/xxx.py`

with docker:
```bash
docker run --rm -it --gpus=all \
    -v "$PWD:/app" \
    -v "$PWD/.kubeconfig:/root/.kube/config" \
    -v "/mnt/c/Users/chenx/.ollama/models:/root/.ollama/models" \
    chenseanxy/llm-ran \
    poetry run python experiments/xxx.py
```

Experiments on CSC:
- `sbatch scripts/download_image.sh`: download and convert docker image to apptainer format
- `sbatch scripts/run_experiment.sh`: submit job
    - -> launches a container with `apptainer exec` once scheduled
        - -> entrypoint being `deployment\apptainer_entry.sh`
            - -> runs `ollama` and `cloudflared` in the background
            - -> runs the command in `scripts/run_experiment.sh` in the foreground
                - -> `experiments/xxx.py`
